{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a510d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import ast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict, Any, Optional\n",
    "import numpy as np\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import datetime\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers.optimization import get_linear_schedule_with_warmup       # AdamW seems no longer available here\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_path = os.path.abspath('')\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "from dataloader_doc_atf import DataManager, DataManagerTest\n",
    "from model_4_doc_atf import MultiModalConcatLineFocalBMESBinaryClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, classification_report\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from dataloader_doc_atf import AtcSidecar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98e889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "with open('./pylint.txt','r') as f:\n",
    "    error_list = f.read()\n",
    "    error_codes = re.findall(r\"\\((\\w\\d{4})\\)\", error_list)\n",
    "    \n",
    "def analyze_pylint_output(eval_result: str) -> Counter:\n",
    "    analysis = [0]*len(error_codes)\n",
    "    error_pattern = re.compile(r\"\\d:\\d+:\\s(\\w\\d{4}):\\s\")\n",
    "    errors = error_pattern.findall(eval_result)\n",
    "\n",
    "    error_counts = Counter(errors)\n",
    "    \n",
    "    analysis = [error_counts[e] for e in error_codes]\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def analyze_pylint_output_line(eval_result: str, total_lines: int):\n",
    "    error_pattern = re.compile(r\"(\\d+):\\d+:\\s(\\w\\d{4}):\\s\")\n",
    "    errors = error_pattern.findall(eval_result)\n",
    "    \n",
    "    line_error_counts = defaultdict(Counter)\n",
    "\n",
    "    for line, code in errors:\n",
    "        line_error_counts[int(line)][code] += 1\n",
    "    \n",
    "    analysis = [[0]*len(error_codes) for _ in range(total_lines)]\n",
    "    \n",
    "    # 각 줄별 에러 코드 카운트를 분석 결과 리스트에 저장\n",
    "    for line in range(total_lines):\n",
    "        if line in line_error_counts:\n",
    "            analysis[line] = [line_error_counts[line][code] for code in error_codes]\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def split_code_sentence(code, use_sp=False):\n",
    "        import re\n",
    "        pattern = re.compile(\n",
    "        r'\"\"\"|\\'\\'\\'|\"|\\'|#|==|'\n",
    "        r'\\n|'\n",
    "        r'[^\\S\\n]+|'\n",
    "        r'\\w+|[.,()\\[\\]{};:\\=\\_\\+\\-\\*\\/\\~\\!\\%\\^\\&\\<\\>\\?]')\n",
    "        \n",
    "        tokens = pattern.findall(code)\n",
    "        return tokens\n",
    "\n",
    "def ccfeature_line_to_token_level(code):\n",
    "    code_tokens = split_code_sentence(code)\n",
    "    count = 0\n",
    "    line_num_list = []\n",
    "    for token in code_tokens:\n",
    "        line_num_list.append(count)\n",
    "        if token == '\\n':\n",
    "            count += 1\n",
    "    return line_num_list[:1024]\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, original_dataset, indices):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.indices = [int(idx) for idx in indices]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        real_idx = self.indices[index]\n",
    "        return self.original_dataset[int(real_idx)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "\n",
    "def get_roc_metrics(true_labels, pred_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, pred_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold=%f, sensitivity = %.3f, specificity = %.3f, J=%.3f' % (best_thresh, tpr[ix], 1-fpr[ix], J[ix]))\n",
    "    return float(roc_auc)\n",
    "\n",
    "class SupervisedTrainer:\n",
    "    def __init__(self, data, model, en_labels, id2label, args):\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.en_labels = en_labels\n",
    "        self.id2label = id2label\n",
    "\n",
    "        self.seq_len = args.seq_len\n",
    "        self.num_train_epochs = args.num_train_epochs\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.warm_up_ratio = args.warm_up_ratio\n",
    "\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self._create_optimizer_and_scheduler()\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_f1_score = 0.0\n",
    "        self.best_model_path = None\n",
    "        self.writer = None\n",
    "        self.loss_function = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        self.threshold = 0.5\n",
    "\n",
    "    def _create_optimizer_and_scheduler(self):\n",
    "        num_training_steps = len(\n",
    "            self.data.train_dataloader) * self.num_train_epochs\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "        named_parameters = self.model.named_parameters()\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in named_parameters\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in named_parameters\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                0.0,\n",
    "            },\n",
    "        ]\n",
    "        self.optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-8,\n",
    "        )\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(self.warm_up_ratio * num_training_steps),\n",
    "            num_training_steps=num_training_steps)\n",
    "\n",
    "    def train(self, ckpt_name='linear_en.pt', prediction_method=\"most_common\"):\n",
    "        \n",
    "        for epoch in trange(int(self.num_train_epochs), desc=\"Epoch\"):\n",
    "            self.model.train()\n",
    "            tr_loss = 0\n",
    "            nb_tr_steps = 0\n",
    "            # train\n",
    "            for step, inputs in enumerate(\n",
    "                    tqdm(self.data.train_dataloader, desc=\"Iteration\")):\n",
    "                # send batch data to GPU\n",
    "                for k, v in inputs.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        inputs[k] = v.to(self.device)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    labels = inputs['labels']\n",
    "                    output = self.model(inputs['features'], inputs['labels'], inputs['ccfeatures'], inputs['atfeatures'])#, inputs['line_indices'])\n",
    "                    logits = output['logits']\n",
    "                    loss = output['loss']\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # print(\"KSY =======================\")\n",
    "                    # for name, p in self.model.named_parameters():\n",
    "                    #     if 'feature_encoder' in name:\n",
    "                    #         print(name)\n",
    "                    #         print(p.grad)\n",
    "                    #         exit()\n",
    "                            \n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                    tr_loss += loss.item()\n",
    "                    nb_tr_steps += 1\n",
    "            \n",
    "                if step % 50 == 0:\n",
    "                    self.writer.add_scalar('Training Loss', loss.item(), epoch * len(self.data.train_dataloader) + step)\n",
    "            \n",
    "            \n",
    "            avg_train_loss = tr_loss / nb_tr_steps\n",
    "            print(f'epoch {epoch+1}: train_loss {avg_train_loss}')\n",
    "            self.writer.add_scalar('Average Training Loss', avg_train_loss, epoch)\n",
    "\n",
    "            # Validate data at the end of every epoch\n",
    "            val_loss, sent_result = self.valid(prediction_method=prediction_method)\n",
    "            self.writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "\n",
    "            # save the best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_path = f\"{ckpt_name}\"\n",
    "                self.writer.add_scalar('Best Validation Loss', self.best_val_loss, epoch)\n",
    "                torch.save(self.model.cpu(), self.best_model_path)\n",
    "                self.model.to(self.device)\n",
    "\n",
    "        # then reload the best model in the end\n",
    "        if self.best_model_path:\n",
    "            print(f\"Reloading best model from {self.best_model_path}\")\n",
    "            self.model.load_state_dict(torch.load(self.best_model_path, weights_only=False).state_dict())\n",
    "            self.model.to(self.device)\n",
    "        \n",
    "        self.writer.close()\n",
    "        return\n",
    "    \n",
    "    def valid(self, content_level_eval=False, prediction_method=\"most_common\"):\n",
    "        self.model.eval()\n",
    "        texts = []\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        total_logits = []\n",
    "        total_probs = []\n",
    "        total_loss = 0.0\n",
    "        total_steps = 0\n",
    "        \n",
    "        for step, inputs in enumerate(\n",
    "                tqdm(self.data.val_dataloader, desc=\"Iteration\")):\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                labels_ = inputs['labels']\n",
    "                output = self.model(inputs['features'], inputs['labels'], inputs['ccfeatures'], inputs['atfeatures'])\n",
    "                preds = output['preds']\n",
    "    \n",
    "                logits_ = output['logits']\n",
    "                \n",
    "                probabilities = F.softmax(logits_, dim=-1)\n",
    "                \n",
    "                logits = logits_.view(-1, logits_.size(-1))\n",
    "                labels = labels_.view(-1)\n",
    "                loss = self.loss_function(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                total_steps += 1\n",
    "\n",
    "                texts.extend(inputs['text'])\n",
    "                pred_labels.extend(preds.cpu().tolist())\n",
    "                true_labels.extend(labels_.cpu().tolist())\n",
    "                total_probs.extend(probabilities)\n",
    "\n",
    "        avg_val_loss = total_loss / total_steps\n",
    "        print(f\"Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        print(\"*\" * 8, \"Sentence Level Evalation\", \"*\" * 8)\n",
    "        #word_result, sent_result = self.sent_level_eval(texts, true_labels, pred_labels, total_probs, prediction_method)\n",
    "        sent_result = self.sent_level_eval(texts, true_labels, pred_labels, total_probs, prediction_method)\n",
    "        \n",
    "        return avg_val_loss, sent_result\n",
    "    \n",
    "    def test(self, test_dataloader, content_level_eval=False, prediction_method=\"most_common\"):\n",
    "        self.model.eval()\n",
    "        texts = []\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        total_logits = []\n",
    "        total_probs = []\n",
    "        problem_ids = []\n",
    "        user_ids = []\n",
    "        \n",
    "        for step, inputs in enumerate(\n",
    "                tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                labels = inputs['labels']\n",
    "                output = self.model(inputs['features'], inputs['labels'], inputs['ccfeatures'], inputs['atfeatures'])#, inputs['line_indices'])\n",
    "                logits = output['logits']\n",
    "                preds = output['preds']\n",
    "                problem_id = inputs['problem_id']\n",
    "                user_id = inputs['user_id']\n",
    "                \n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "                texts.extend(inputs['text'])\n",
    "                pred_labels.extend(preds.cpu().tolist())\n",
    "                true_labels.extend(labels.cpu().tolist())\n",
    "                problem_ids.extend(problem_id)\n",
    "                user_ids.extend(user_id)\n",
    "                total_logits.extend(logits.cpu().tolist())\n",
    "                total_probs.extend(probabilities)\n",
    "        \n",
    "        line_counts = [len(text.split('\\n')) for text in texts]\n",
    "        \n",
    "        if content_level_eval:\n",
    "            # content level evaluation\n",
    "            print(\"*\" * 8, \"Content Level Evalation\", \"*\" * 8)\n",
    "            content_result = self.content_level_eval(texts, true_labels, pred_labels, total_probs, prediction_method)\n",
    "        else:\n",
    "            content_result = None\n",
    "        print(\"*\" * 8, \"Sentence Level Evalation\", \"*\" * 8)\n",
    "        #word_result, sent_result = self.sent_level_eval(texts, true_labels, pred_labels, total_probs, prediction_method)\n",
    "        sent_result = self.sent_level_eval(texts, true_labels, pred_labels, total_probs, prediction_method)\n",
    "            \n",
    "        # return sent_result, content_result, {'text':texts,'pred':pred_labels, 'true':true_labels, 'problem_id':problem_ids, 'user_id': user_ids}\n",
    "        return sent_result, content_result, {'text': texts, 'pred': pred_labels, 'true': true_labels, 'problem_id':problem_ids, 'user_id':user_ids, 'line_count':line_counts}\n",
    "\n",
    "    \n",
    "    def content_level_eval(self, texts, true_labels, pred_labels, pred_probs, prediction_method='most_common'):\n",
    "        if prediction_method =='threshold':\n",
    "            threshold = self.threshold\n",
    "        else:\n",
    "            threshold = None\n",
    "            pred_labels_threshold = pred_labels\n",
    "        \n",
    "        true_content_labels = []\n",
    "        pred_content_labels = []\n",
    "        pred_content_probs = []\n",
    "        \n",
    "        for text, true_label, pred_label, pred_prob in zip(texts, true_labels, pred_labels_threshold, pred_probs):\n",
    "            true_label = np.array(true_label)\n",
    "            pred_label = np.array(pred_label)\n",
    "            pred_prob = np.array(pred_prob.cpu())\n",
    "            \n",
    "            mask = true_label != -1\n",
    "            true_label = true_label[mask].tolist()\n",
    "            pred_label = pred_label[mask].tolist()\n",
    "            \n",
    "            pred_prob = torch.tensor(pred_prob[mask])\n",
    "            true_common_tag = self._get_most_common_tag(true_label)\n",
    "            true_content_labels.append(true_common_tag[0])\n",
    "            \n",
    "            pred_common_tag = self._get_most_common_tag(pred_label)\n",
    "            pred_content_labels.append(pred_common_tag[0])\n",
    "            \n",
    "            cont_prob = pred_prob[:, 4:8].sum(dim=1)\n",
    "            pred_content_prob = torch.mean(cont_prob, dim=0)\n",
    "            pred_content_probs.append(pred_content_prob.item())\n",
    "            \n",
    "        true_content_labels = [self.en_labels[label] for label in true_content_labels]\n",
    "        pred_content_labels = [self.en_labels[label] for label in pred_content_labels]\n",
    "        \n",
    "        result = self._get_precision_recall_acc_f1(true_content_labels, pred_content_labels, pred_content_probs)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def sent_level_eval(self, texts, true_labels, pred_labels, pred_probs, prediction_method='most_common'):\n",
    "        if prediction_method =='threshold':\n",
    "            threshold = self.threshold\n",
    "        else:\n",
    "            threshold = None\n",
    "            pred_labels_threshold = pred_labels\n",
    "        \n",
    "        # For line-wise labeling\n",
    "        true_sent_labels = []\n",
    "        pred_sent_labels = []\n",
    "        pred_sent_probs = []\n",
    "        for text, true_label, pred_label, pred_prob in zip(texts, true_labels, pred_labels_threshold, pred_probs):\n",
    "            true_label = np.array(true_label)\n",
    "            pred_label = np.array(pred_label)\n",
    "            pred_prob = np.array(pred_prob.cpu())\n",
    "            mask = true_label != -1\n",
    "            true_label = true_label[mask].tolist()\n",
    "            pred_label = pred_label[mask].tolist()\n",
    "            pred_prob = torch.tensor(pred_prob[mask])\n",
    "            sents = text.split('\\n')\n",
    "            for true_label_idx in range(len(true_label)):\n",
    "                if sents[true_label_idx] == '' or sents[true_label_idx].isspace():  # 빈 문장일 경우 처리하지 않음\n",
    "                    continue\n",
    "                true_sent_label = self.id2label[true_label[true_label_idx]]\n",
    "                pred_sent_label = self.id2label[pred_label[true_label_idx]]\n",
    "                \n",
    "                true_sent_labels.append(true_sent_label.split('-')[-1])\n",
    "                pred_sent_prob = pred_prob[true_label_idx, 4:8].sum()\n",
    "                pred_sent_probs.append(pred_sent_prob.item())\n",
    "                pred_sent_labels.append(pred_sent_label.split('-')[-1])\n",
    "            \n",
    "        true_sent_labels = [self.en_labels[label] for label in true_sent_labels]\n",
    "        pred_sent_labels = [self.en_labels[label] for label in pred_sent_labels]\n",
    "        \n",
    "        sent_result = self._get_precision_recall_acc_f1(true_sent_labels, pred_sent_labels, pred_sent_probs)\n",
    "        return sent_result\n",
    "    \n",
    "    \n",
    "    def _get_threshold_tag(self, logits, machine_threshold=0.5):\n",
    "        human_logits = logits[:, :, :4]  # Human Classes\n",
    "        machine_logits = logits[:, :, 4:] # Machine Classes\n",
    "        human_scores = torch.sum(human_logits, dim=-1)  # Shape: [batch_size, seq_len]\n",
    "        machine_scores = torch.sum(machine_logits, dim=-1)        # Shape: [batch_size, seq_len]\n",
    "        pred_labels = torch.where(machine_scores >= machine_threshold, 4, 0)  # 0 for Human, 4 for AI\n",
    "        \n",
    "        return pred_labels.cpu().tolist()\n",
    "    \n",
    "    def _get_most_common_tag(self, tags):\n",
    "        \"\"\"most_common_tag is a tuple: (tag, times)\"\"\"\n",
    "        from collections import Counter\n",
    "        tags = [self.id2label[tag] for tag in tags]\n",
    "        tags = [tag.split('-')[-1] for tag in tags]\n",
    "        tag_counts = Counter(tags)\n",
    "        most_common_tag = tag_counts.most_common(1)[0]\n",
    "        return most_common_tag\n",
    "    \n",
    "    def _get_precision_recall_acc_f1(self, true_labels, pred_labels, pred_probs=None, pos_label: int = 1) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        true_labels: [0/1]\n",
    "        pred_labels: 이미 threshold가 적용된 0/1 예측\n",
    "        pred_probs : 선택. 점수(양성=pos_label의 확률/로짓 등). 있으면 ROC/AUPRC과 임계값 탐색 리포트 추가.\n",
    "        pos_label  : 양성 클래스(기본 1)\n",
    "        \"\"\"\n",
    "        y_true = np.asarray(true_labels).astype(int)\n",
    "        y_pred = np.asarray(pred_labels).astype(int)\n",
    "\n",
    "        # --- 기본 리포트(주어진 라벨 기준) ---\n",
    "        acc  = accuracy_score(y_true, y_pred)\n",
    "        mF1  = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        bF1  = f1_score(y_true, y_pred, average='binary', pos_label=pos_label, zero_division=0)\n",
    "        prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        rec  = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "        cm   = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "        print(\"=== Given labels (as-is) ===\")\n",
    "        print(\"Accuracy: {:.3f}\".format(acc*100))\n",
    "        print(\"Macro F1 Score: {:.3f}\".format(mF1*100))\n",
    "        print(\"Binary F1 Score (pos): {:.3f}\".format(bF1*100))\n",
    "        print(\"Precision/Recall per class:\")\n",
    "        print(\"{:.1f},{:.1f},{:.1f},{:.1f}\".format(prec[0]*100, rec[0]*100, prec[1]*100, rec[1]*100))\n",
    "        print(f\"CM [[TN FP],[FN TP]] = {cm.tolist()}\")\n",
    "\n",
    "        # 결과 dict 시작\n",
    "        result: Dict[str, Any] = {\n",
    "            \"given_labels\": {\n",
    "                \"accuracy\": acc, \"macro_f1\": mF1, \"binary_f1\": bF1,\n",
    "                \"precision\": prec, \"recall\": rec, \"cm\": cm\n",
    "            },\n",
    "            \"roc_auc\": None,\n",
    "            \"auprc\": None,\n",
    "            \"thresholds\": {}\n",
    "        }\n",
    "\n",
    "        # --- 점수 기반 추가 리포트 ---\n",
    "        if pred_probs is not None:\n",
    "            y_score = np.asarray(pred_probs, dtype=float)\n",
    "\n",
    "            # ROC / AUPRC\n",
    "            try:\n",
    "                fpr, tpr, thr_roc = roc_curve(y_true, y_score, pos_label=pos_label)\n",
    "                roc_auc = float(auc(fpr, tpr))\n",
    "            except Exception:\n",
    "                roc_auc = None\n",
    "\n",
    "            try:\n",
    "                auprc = float(average_precision_score(y_true, y_score, pos_label=pos_label))\n",
    "            except Exception:\n",
    "                auprc = None\n",
    "\n",
    "            print(f\"ROC_AUC (fpr-tpr): {roc_auc:.3f}\" if roc_auc is not None else \"ROC_AUC: N/A\")\n",
    "            print(f\"AUPRC: {auprc:.3f}\" if auprc is not None else \"AUPRC: N/A\")\n",
    "\n",
    "            # Helper: 특정 threshold에서 평가\n",
    "            def eval_at(thr: float, tag: str) -> Dict[str, Any]:\n",
    "                y_hat = (y_score > thr).astype(int)\n",
    "                acc_  = accuracy_score(y_true, y_hat)\n",
    "                mF1_  = f1_score(y_true, y_hat, average='macro', zero_division=0)\n",
    "                bF1_  = f1_score(y_true, y_hat, average='binary', pos_label=pos_label, zero_division=0)\n",
    "                pr_   = precision_score(y_true, y_hat, average=None, zero_division=0)\n",
    "                rc_   = recall_score(y_true, y_hat, average=None, zero_division=0)\n",
    "                cm_   = confusion_matrix(y_true, y_hat, labels=[0,1])\n",
    "                print(f\"[{tag}] thr={thr:.3f} | Acc={acc_*100:.1f}  MacroF1={mF1_*100:.1f}  BinF1(pos)={bF1_*100:.1f}\")\n",
    "                print(\" P/R per class -> 0(H): {:.1f}/{:.1f} , 1(AI): {:.1f}/{:.1f}\".format(pr_[0]*100, rc_[0]*100, pr_[1]*100, rc_[1]*100))\n",
    "                print(f\" CM [[TN FP],[FN TP]] = {cm_.tolist()}\")\n",
    "                return {\"thr\": float(thr), \"accuracy\": acc_, \"macro_f1\": mF1_, \"binary_f1\": bF1_, \"precision\": pr_, \"recall\": rc_, \"cm\": cm_}\n",
    "\n",
    "            # Youden J (TPR - FPR) 최대\n",
    "            def best_thr_youden() -> float:\n",
    "                if roc_auc is None or len(thr_roc) == 0:\n",
    "                    return 0.5\n",
    "                J = tpr - fpr\n",
    "                i = int(np.argmax(J))\n",
    "                return float(thr_roc[i])\n",
    "\n",
    "            # 양성 F1 최대(PR 기반)\n",
    "            def best_thr_posF1() -> float:\n",
    "                prec_curve, rec_curve, thr_pr = precision_recall_curve(y_true, y_score, pos_label=pos_label)\n",
    "                if len(thr_pr) == 0:\n",
    "                    return 0.5\n",
    "                f1_curve = (2 * prec_curve * rec_curve) / (prec_curve + rec_curve + 1e-12)\n",
    "                i = int(np.nanargmax(f1_curve[:-1]))  # 마지막 점은 threshold 없음\n",
    "                return float(thr_pr[i])\n",
    "\n",
    "            thr05     = 0.5\n",
    "            thrJ      = best_thr_youden()\n",
    "            thrBestF1 = best_thr_posF1()\n",
    "\n",
    "            print(\"=== Threshold sweeps on scores ===\")\n",
    "            res05  = eval_at(thr05, \"thr=0.5\")\n",
    "            resJ   = eval_at(thrJ, \"thr=YoudenJ\")\n",
    "            resF1  = eval_at(thrBestF1, \"thr=bestPosF1\")\n",
    "\n",
    "            result.update({\n",
    "                \"roc_auc\": roc_auc,\n",
    "                \"auprc\": auprc,\n",
    "                \"thresholds\": {\n",
    "                    \"thr@0.5\": res05,\n",
    "                    \"thr@youden\": resJ,\n",
    "                    \"thr@best_posF1\": resF1\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            print(\"ROC_AUC (fpr-tpr): N/A (pred_probs is None)\")\n",
    "            print(\"AUPRC: N/A (pred_probs is None)\")\n",
    "\n",
    "        # CSV 한 줄 요약(기존 포맷과 유사)\n",
    "        pr_line = \"{:.1f},{:.1f},{:.1f},{:.1f}\".format(prec[0]*100, rec[0]*100, prec[1]*100, rec[1]*100)\n",
    "        print(\"{:.1f},{:.1f},{:.1f},{},{:.3f},{}\".format(\n",
    "            acc*100, mF1*100, bF1*100, pr_line, result[\"roc_auc\"] if result[\"roc_auc\"] is not None else float(\"nan\"),\n",
    "            f\"{result['auprc']:.3f}\" if result[\"auprc\"] is not None else \"N/A\"\n",
    "        ))\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def construct_bmes_labels(labels):\n",
    "    prefix = ['B-', 'M-', 'E-', 'S-']\n",
    "    id2label = {}\n",
    "    counter = 0\n",
    "\n",
    "    for label, id in labels.items():\n",
    "        for pre in prefix:\n",
    "            id2label[counter] = pre + label\n",
    "            counter += 1\n",
    "    \n",
    "    return id2label\n",
    "\n",
    "def remove_duplicates(prob_dict):\n",
    "    total_p = 0\n",
    "    total = 0\n",
    "    for problem_id, entries in prob_dict.items():\n",
    "        n = 0\n",
    "        unique_texts = set()\n",
    "        unique_entries = []\n",
    "        \n",
    "        for entry in entries:\n",
    "            if entry['text'] not in unique_texts:\n",
    "                unique_entries.append(entry)\n",
    "                unique_texts.add(entry['text'])\n",
    "            else:\n",
    "                n += 1\n",
    "        if n != 0:\n",
    "            total_p += 1\n",
    "        total += n\n",
    "        \n",
    "        prob_dict[problem_id] = unique_entries     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d686ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def warn_group_overlap(groups_arr, idx_a, idx_b, name_a=\"A\", name_b=\"B\"):\n",
    "    ga = set(groups_arr[idx_a])\n",
    "    gb = set(groups_arr[idx_b])\n",
    "    inter = ga & gb\n",
    "    if inter:\n",
    "        print(f\"[WARN] {name_a} and {name_b} share {len(inter)} problem_ids (leak risk).\")\n",
    "    else:\n",
    "        print(f\"[OK] No problem_id overlap between {name_a} and {name_b}.\")\n",
    "\n",
    "def split_dataset(data_path, dataset, seed=42, test_size=0.2, val_size=0.1):\n",
    "    # 1) Load full set\n",
    "    with open(os.path.join(data_path, f\"{dataset}_features.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        #full_train_set = [json.loads(line) for line in f]\n",
    "\n",
    "        full_train_set = []\n",
    "        for line in f:\n",
    "            dumped_line = json.loads(line)\n",
    "            dumped_line[\"user_id\"] = \"\"\n",
    "            if dumped_line[\"LLM\"] == \"Human\":\n",
    "                dumped_line[\"label_int\"] = 0\n",
    "            else:\n",
    "                dumped_line[\"label_int\"] = 1\n",
    "\n",
    "            full_train_set.append(dumped_line)\n",
    "\n",
    "\n",
    "\n",
    "    # full_train_set = [x for x in full_train_set if x.get(\"LLM\") != \"GPT3.5\" and x.get(\"LLM\") != \"GEMINI\"]\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # 2) Build features (pylint 기반)\n",
    "    for i, sample in enumerate(full_train_set):\n",
    "        # problem_id가 없을 수도 있으니 안전하게 기본값\n",
    "        if sample.get(\"problem_id\") is None:\n",
    "            sample[\"problem_id\"] = f\"__none__#{i}\"\n",
    "\n",
    "        if 'line' in dataset:\n",
    "            n_lines = len(sample.get('text', '').split('\\n'))\n",
    "            ccfeature_line = analyze_pylint_output_line(sample.get('eval', ''), n_lines)\n",
    "            sample['ccfeature'] = ccfeature_line\n",
    "        else:\n",
    "            sample['ccfeature'] = analyze_pylint_output(sample.get('eval', ''))\n",
    "\n",
    "    # 3) Arrays for splitting\n",
    "    labels = np.array([sample['label'] for sample in full_train_set])\n",
    "    groups = np.array([sample['problem_id'] for sample in full_train_set])\n",
    "\n",
    "    # 4) Group-aware Train/Test split\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    train_full_idx, test_idx = next(\n",
    "        gss.split(\n",
    "            np.zeros(len(full_train_set)),\n",
    "            labels,\n",
    "            groups=groups\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 5) Group-aware Train/Val split (within train_full)\n",
    "    gss_val = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
    "    train_idx, val_idx = next(\n",
    "        gss_val.split(\n",
    "            np.zeros(len(train_full_idx)),\n",
    "            labels[train_full_idx],\n",
    "            groups=groups[train_full_idx]\n",
    "        )\n",
    "    )\n",
    "    # 인덱스를 원본 기준으로 변환\n",
    "    train_idx = train_full_idx[train_idx]\n",
    "    val_idx   = train_full_idx[val_idx]\n",
    "\n",
    "    # 6) 누수(그룹 겹침) 점검\n",
    "    warn_group_overlap(groups, train_idx, val_idx, \"Train\", \"Val\")\n",
    "    warn_group_overlap(groups, train_idx, test_idx, \"Train\", \"Test\")\n",
    "    warn_group_overlap(groups, val_idx,   test_idx, \"Val\",   \"Test\")\n",
    "\n",
    "    # 7) 실제 세트 구성\n",
    "    train_set = [full_train_set[i] for i in train_idx]\n",
    "    val_set   = [full_train_set[i] for i in val_idx]\n",
    "    test_set  = [full_train_set[i] for i in test_idx]\n",
    "\n",
    "    # 8) 라벨 분포 확인(옵션이지만 유용)\n",
    "    def distrib(name, arr):\n",
    "        c = Counter([s['label'] for s in arr])\n",
    "        total = len(arr)\n",
    "        print(f\"{name}: {total}  | human={c.get('human',0)} ({c.get('human',0)/total:.2%}), AI={c.get('AI',0)} ({c.get('AI',0)/total:.2%})\")\n",
    "\n",
    "    print(f\"Train: {len(train_set)}, Validation: {len(val_set)}, Test: {len(test_set)}\")\n",
    "    distrib(\"Train\", train_set)\n",
    "    distrib(\"Val\",   val_set)\n",
    "    distrib(\"Test\",  test_set)\n",
    "    \n",
    "    return [train_set, val_set, test_set]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1501f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default='Transformer')\n",
    "    parser.add_argument('--gpu', type=str, default='0')\n",
    "    parser.add_argument('--train_mode', type=str, default='classify')\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--seq_len', type=int, default=1024)\n",
    "    parser.add_argument('--dataset', type=str, default=\"\")\n",
    "    parser.add_argument('--method', type=str, default=\"focalbmesbinary_embedconcat_transformer256\")\n",
    "    \n",
    "    parser.add_argument('--train_ratio', type=float, default=0.9)\n",
    "    parser.add_argument('--split_dataset', action='store_true')\n",
    "    parser.add_argument('--data_path', type=str, default='')\n",
    "    parser.add_argument('--train_path', type=str, default='')\n",
    "    parser.add_argument('--valid_path', type=str, default='')\n",
    "    parser.add_argument('--test_path', type=str, default='')\n",
    "\n",
    "    parser.add_argument('--num_train_epochs', type=int, default=20)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.1)\n",
    "    parser.add_argument('--lr', type=float, default=5e-5)\n",
    "    parser.add_argument('--warm_up_ratio', type=float, default=0.1)\n",
    "    parser.add_argument('--seed', type=int, default=42, required=True)\n",
    "    parser.add_argument('--do_test', action='store_true')\n",
    "    parser.add_argument('--test_content', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--ckpt_name', type=str, default='')\n",
    "    parser.add_argument('--alpha', type=float, default=0.5)\n",
    "    parser.add_argument('--testbed', type=str, required=True)\n",
    "\n",
    "    parser.add_argument('--at_feature_path', type=str, default='')\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba197f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log INFO: split dataset...\n",
      "[OK] No problem_id overlap between Train and Val.\n",
      "[OK] No problem_id overlap between Train and Test.\n",
      "[OK] No problem_id overlap between Val and Test.\n",
      "Train: 1992, Validation: 231, Test: 564\n",
      "Train: 1992  | human=0 (0.00%), AI=1992 (100.00%)\n",
      "Val: 231  | human=0 (0.00%), AI=231 (100.00%)\n",
      "Test: 564  | human=0 (0.00%), AI=564 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [00:01<00:00, 1673.09it/s]\n",
      "100%|██████████| 231/231 [00:00<00:00, 3010.43it/s]\n",
      "100%|██████████| 564/564 [00:00<00:00, 2001.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log INFO: do test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 18/18 [00:19<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Sentence Level Evalation ********\n",
      "=== Given labels (as-is) ===\n",
      "Accuracy: 77.486\n",
      "Macro F1 Score: 71.148\n",
      "Binary F1 Score (pos): 57.624\n",
      "Precision/Recall per class:\n",
      "75.9,95.7,84.6,43.7\n",
      "CM [[TN FP],[FN TP]] = [[7921, 355], [2513, 1950]]\n",
      "ROC_AUC (fpr-tpr): 0.853\n",
      "AUPRC: 0.767\n",
      "=== Threshold sweeps on scores ===\n",
      "[thr=0.5] thr=0.500 | Acc=76.5  MacroF1=69.7  BinF1(pos)=55.4\n",
      " P/R per class -> 0(H): 75.2/95.3 , 1(AI): 82.8/41.6\n",
      " CM [[TN FP],[FN TP]] = [[7891, 385], [2607, 1856]]\n",
      "[thr=YoudenJ] thr=0.256 | Acc=76.7  MacroF1=75.3  BinF1(pos)=69.5\n",
      " P/R per class -> 0(H): 85.5/77.2 , 1(AI): 64.2/75.7\n",
      " CM [[TN FP],[FN TP]] = [[6393, 1883], [1086, 3377]]\n",
      "[thr=bestPosF1] thr=0.256 | Acc=76.7  MacroF1=75.3  BinF1(pos)=69.5\n",
      " P/R per class -> 0(H): 85.5/77.2 , 1(AI): 64.2/75.7\n",
      " CM [[TN FP],[FN TP]] = [[6393, 1883], [1086, 3377]]\n",
      "77.5,71.1,57.6,75.9,95.7,84.6,43.7,0.853,0.767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sys.argv = [\n",
    "        \"train.py\",\n",
    "        \"--dataset\", \"codenet(python)_gemini_hybrid_line\",\n",
    "        \"--data_path\", \"./data\",\n",
    "        \"--seed\", \"42\",\n",
    "        \"--testbed\", \"toplevel\",\n",
    "        \"--ckpt_name\", \"codenet(python)_gemini_hybrid_line_docatf\",\n",
    "        \"--do_test\",\n",
    "    ]\n",
    "\n",
    "    args = parse_args()\n",
    "    \n",
    "    print(\"Log INFO: split dataset...\")\n",
    "    df_ = split_dataset(data_path=args.data_path, seed=args.seed, dataset=args.dataset)  # [train, val, test]\n",
    "\n",
    "    en_labels = {\n",
    "        'human': 0,\n",
    "        'AI': 1\n",
    "    }\n",
    "    \n",
    "    id2label = construct_bmes_labels(en_labels)\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    prediction_method = 'most_common'\n",
    "\n",
    "    experiment_results = []\n",
    "\n",
    "    if 'revised' in args.dataset:\n",
    "        at_sidecar = AtcSidecar('./limo_atf/great_data_doc/index.json')\n",
    "        datas = DataManagerTest(datas=df_, batch_size=args.batch_size, max_len=args.seq_len, human_label='human', id2label=id2label, at_feature_lookup=at_sidecar)\n",
    "    else:\n",
    "        at_sidecar = AtcSidecar('./limo_atf/great_data_doc/index.json')\n",
    "        datas = DataManager(datas=df_, batch_size=args.batch_size, max_len=args.seq_len, human_label='human', id2label=id2label, at_feature_lookup=at_sidecar)\n",
    "\n",
    "    # classifier 선택\n",
    "    if args.method == 'focalbmesbinary_embedconcat_transformer256':\n",
    "        if args.testbed == 'toplevel':\n",
    "            if 'gemini' in args.dataset or 'gpt4' in args.dataset:\n",
    "                classifier = MultiModalConcatLineFocalBMESBinaryClassifier(id2labels=id2label, seq_len=args.seq_len, alpha=args.alpha)\n",
    "\n",
    "    ckpt_name = f'ckpt/{args.ckpt_name}_best_f1.pt'\n",
    "\n",
    "    trainer = SupervisedTrainer(datas, classifier, en_labels, id2label, args)\n",
    "    trainer.writer = SummaryWriter(log_dir=f\"runs/python_{args.ckpt_name}\")\n",
    "\n",
    "    experiment_result = {}\n",
    "\n",
    "    if args.do_test:\n",
    "        print(\"Log INFO: do test...\")\n",
    "        saved_model = torch.load(ckpt_name, weights_only=False)\n",
    "        trainer.model.load_state_dict(saved_model.state_dict())\n",
    "        if 'hybrid' in args.dataset or 'revised' in args.dataset:\n",
    "            test_sent_result, _, test_raw_results = trainer.test(datas.test_dataloader, content_level_eval=False, prediction_method=prediction_method)\n",
    "            experiment_result['test_result'] = {'line': test_sent_result, 'raw': test_raw_results}\n",
    "        else:\n",
    "            test_sent_result, test_content_result, test_raw_results = trainer.test(datas.test_dataloader, content_level_eval=True, prediction_method=prediction_method)\n",
    "            experiment_result['test_result'] = {'line': test_sent_result, 'document': test_content_result, 'raw': test_raw_results}\n",
    "    else:\n",
    "        print(\"Log INFO: do train...\")\n",
    "        trainer.train(ckpt_name=ckpt_name, prediction_method=prediction_method)\n",
    "\n",
    "        if 'hybrid' in args.dataset or 'revised' in args.dataset:\n",
    "            test_sent_result, _, test_raw_results = trainer.test(datas.test_dataloader, content_level_eval=False, prediction_method=prediction_method)\n",
    "            experiment_result['test_result'] = {'line': test_sent_result, 'raw': test_raw_results}\n",
    "        else:\n",
    "            test_sent_result, test_content_result, test_raw_results = trainer.test(datas.test_dataloader, content_level_eval=True, prediction_method=prediction_method)\n",
    "            experiment_result['test_result'] = {'line': test_sent_result, 'document': test_content_result, 'raw': test_raw_results}\n",
    "\n",
    "    experiment_results.append(experiment_result)\n",
    "\n",
    "    with open(f'result/experiment_results_{args.ckpt_name}.json', 'w') as file:\n",
    "        json.dump(experiment_results, file, ensure_ascii=False, cls=NpEncoder)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
