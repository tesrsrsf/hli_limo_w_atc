{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e357ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Jupyter-friendly version of build_atc_features.py\n",
    "Safe incremental execution, same logic as original.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, re, time, hashlib, random, logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import random as pyrandom\n",
    "import pickle\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "BASE_JSONL_PATH = os.environ.get(\"BASE_JSONL_PATH\", \"./data/codenet(python)_gemini_hybrid_line_features.jsonl\")\n",
    "\n",
    "OUTPUT_JSONL_PATH = os.environ.get(\"OUTPUT_JSONL_PATH\", \"./limo_atf/segment_python_at_features.jsonl\")\n",
    "OUTPUT_RAW_EMBED_PATH = os.environ.get(\"OUTPUT_RAW_EMBED_PATH\", \"./limo_atf/segment_python_atfeature_raw_embeddings.json\") # save raw embeddings before PCA\n",
    "\n",
    "PCA_PICKLE_PATH = os.environ.get(\"PCA_PICKLE_PATH\", \"./pca/segment_python_atfeature_pca.pkl\")\n",
    "PCA_META_PATH = os.environ.get(\"PCA_META_PATH\", \"./pca/segment_python_atfeature_pca_meta.json\")\n",
    "\n",
    "TRAIN_DOC_KEYS_FILE = os.environ.get(\"TRAIN_DOC_KEYS_FILE\", \"\") # optional: a txt file with doc_keys used to fit PCA\n",
    "\n",
    "# Models\n",
    "LLM_MODEL_NAME = os.environ.get(\"LLM_MODEL_NAME\", \"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "EMBED_MODEL_NAME = os.environ.get(\"EMBED_MODEL_NAME\", \"microsoft/codebert-base\")\n",
    "\n",
    "# Generation / Embedding\n",
    "N_TASKS_PER_SEGMENT = int(os.environ.get(\"N_TASKS_PER_SEGMENT\", \"1\")) # usually 1 is enough\n",
    "LLM_MAX_NEW_TOKENS = int(os.environ.get(\"LLM_MAX_NEW_TOKENS\", \"128\"))\n",
    "LLM_TEMPERATURE = float(os.environ.get(\"LLM_TEMPERATURE\", \"0.3\"))\n",
    "EMBED_MAX_LEN = int(os.environ.get(\"EMBED_MAX_LEN\", \"256\"))\n",
    "\n",
    "# PCA\n",
    "PCA_DIM = int(os.environ.get(\"PCA_DIM\", \"128\"))\n",
    "\n",
    "# Segmentation\n",
    "SEGMENT_WINDOW_K = int(os.environ.get(\"SEGMENT_WINDOW_K\", \"0\")) # context segments on each side for prompting\n",
    "MIN_SEG_LINES = int(os.environ.get(\"MIN_SEG_LINES\", \"1\")) # merge tiny segments if needed\n",
    "\n",
    "# Randomness / logging\n",
    "SEED = int(os.environ.get(\"SEED\", \"1234\"))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "LANGUAGE_HINT = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b737c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha1_text(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "\n",
    "def smart_doc_key(rec: Dict[str, Any]) -> str:\n",
    "    pid = str(rec.get(\"problem_id\", \"\"))\n",
    "    did = rec.get(\"doc_id\")\n",
    "    sha = rec.get(\"text_sha1\")\n",
    "    if not sha:\n",
    "        txt = rec.get(\"text\") or \"\"\n",
    "        if txt:\n",
    "            sha = sha1_text(txt)\n",
    "    if did:\n",
    "        return f\"{pid}::{did}\"\n",
    "    if sha:\n",
    "        return f\"{pid}::{sha}\"\n",
    "    return f\"{pid}::row{rec.get('_row_id','')}\"\n",
    "\n",
    "\n",
    "def iter_jsonl(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            obj.setdefault(\"_row_id\", i)\n",
    "            yield obj\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def append_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskGenerator:\n",
    "    \"\"\"LLM that turns a code **segment** into a one-sentence approximate task.\n",
    "\n",
    "    CHANGED: prompt is now segment-centric; removed target-line wording.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # slight, notebook-friendly defaults; adjust if you have larger GPUs\n",
    "        max_memory = {0: \"20GiB\"}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        # Avoid the spammy warning by ensuring pad_token_id is set\n",
    "        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.gen = torch.Generator(device=str(self.model.device)).manual_seed(SEED)\n",
    "\n",
    "    @staticmethod\n",
    "    def prompt_segment(segment_text: str, language_hint: Optional[str] = None, context_text: str = \"\") -> str:\n",
    "        lang = language_hint or \"\"\n",
    "        prog_ctx = f\"\\n<program_context>\\n{context_text}\\n</program_context>\\n\" if context_text else \"\\n\"\n",
    "        return (\n",
    "            \"You are a careful code reviewer analyzing a full program.\\n\"\n",
    "            \"Your goal is to **infer the design intent** behind each code segment, not to restate its actions.\\n\"\n",
    "            \"Read the overall program to understand its purpose, then hypothesize **why the TARGET SEGMENT exists**, what issue it solves, or what design goal it fulfills.\\n\\n\"\n",
    "            \"Output: ONE concise English sentence (<=30 words) describing the segment\\'s intent or motivation within the program.\\n\"\n",
    "            \"Prefer action verbs (e.g., validate, parse, dispatch, fetch, cache, format). If context is insufficient, make a\\n\"\n",
    "            \"best-effort guess based on available information.\\n\"\n",
    "            \"Avoid implementation detail or pseudocode; focus on purpose and reasoning.\\n\\n\"\n",
    "            f\"<language>{lang}</language>\\n\"\n",
    "            f\"{prog_ctx}\"\n",
    "            f\"<target_segment>\\n{segment_text}\\n</target_segment>\\n\"\n",
    "        )\n",
    "\n",
    "    def wrap(self, user_text: str) -> str:\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\") and callable(self.tokenizer.apply_chat_template):\n",
    "            msgs = [{\"role\": \"user\", \"content\": user_text}]\n",
    "            return self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        return f\"[INST] {user_text.strip()} [/INST]\"\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def gen_tasks(self, segment_text: str, language_hint: Optional[str] = None, context_text: str = \"\") -> List[str]:\n",
    "        prompt = self.prompt_segment(segment_text, language_hint, context_text)\n",
    "        full = self.wrap(prompt)\n",
    "        in_ids = self.tokenizer(full, return_tensors=\"pt\").to(self.model.device)\n",
    "        out = self.model.generate(\n",
    "            **in_ids,\n",
    "            do_sample=True,\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            max_new_tokens=LLM_MAX_NEW_TOKENS,\n",
    "            num_return_sequences=N_TASKS_PER_SEGMENT,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        texts = self.tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "        # Keep only the assistant parts after the wrap (simple split)\n",
    "        results = []\n",
    "        for s in texts:\n",
    "            if \"### Response:\" in s:\n",
    "                s = s.split(\"### Response:\", 1)[-1]\n",
    "            elif \"[/INST]\" in s:\n",
    "                s = s.split(\"[/INST]\", 1)[-1]\n",
    "\n",
    "            s = s.strip()\n",
    "            # Sanitise into one line\n",
    "            s = re.sub(r\"\\s+\", \" \", s)\n",
    "            \n",
    "            if s:\n",
    "                results.append(s)\n",
    "        # Deduplicate while preserving order\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for t in results:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                uniq.append(t)\n",
    "        return uniq[: N_TASKS_PER_SEGMENT]\n",
    "\n",
    "# %%\n",
    "# --- Task Embedder (CodeBERT) ----------------------------------------------\n",
    "class TaskEmbedder:\n",
    "    def __init__(self):\n",
    "        max_memory = {0: \"12GiB\"}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME, use_fast=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            EMBED_MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        enc = self.tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=EMBED_MAX_LEN, return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        out = self.model(**enc)\n",
    "        last = out.last_hidden_state\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)\n",
    "        summed = (last * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1)\n",
    "        mean_pool = summed / denom\n",
    "        return mean_pool.float().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenter\n",
    "\n",
    "BLOCK_START_RE = re.compile(r\"^(\\s*)(def |class |if |for |while |try:|with |switch\\b|function\\b|public |private |protected |@) | .*\\{\\s*$\", re.X)\n",
    "\n",
    "\n",
    "def _is_blank(s: str) -> bool:\n",
    "    return len(s.strip()) == 0\n",
    "\n",
    "\n",
    "def _brace_delta(s: str) -> int:\n",
    "    # naive balance for C-like braces; good enough for heuristic segmentation\n",
    "    opens = s.count(\"{\")\n",
    "    closes = s.count(\"}\")\n",
    "    return opens - closes\n",
    "\n",
    "\n",
    "def segment_code(lines: List[str], lang_hint: Optional[str] = None, min_seg_lines: int = 1) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Return a list of (start_idx, end_idx) for code segments.\n",
    "\n",
    "    Heuristic: split by blank lines, but keep blocks together until brace depth returns to 0.\n",
    "    This is language-agnostic and robust enough for mixed repos.\n",
    "    \"\"\"\n",
    "    n = len(lines)\n",
    "    segs: List[Tuple[int, int]] = []\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        # skip leading blanks\n",
    "        while i < n and _is_blank(lines[i]):\n",
    "            i += 1\n",
    "        if i >= n:\n",
    "            break\n",
    "        start = i\n",
    "        depth = 0\n",
    "        j = i\n",
    "        while j < n:\n",
    "            s = lines[j]\n",
    "            depth += _brace_delta(s)\n",
    "            # if blank and depth==0 -> boundary\n",
    "            if _is_blank(s) and depth == 0 and j > start:\n",
    "                # trim trailing blanks\n",
    "                k = j - 1\n",
    "                while k >= start and _is_blank(lines[k]):\n",
    "                    k -= 1\n",
    "                if k >= start:\n",
    "                    segs.append((start, k))\n",
    "                j += 1\n",
    "                break\n",
    "            j += 1\n",
    "        else:\n",
    "            # reached EOF\n",
    "            k = n - 1\n",
    "            while k >= start and _is_blank(lines[k]):\n",
    "                k -= 1\n",
    "            if k >= start:\n",
    "                segs.append((start, k))\n",
    "            break\n",
    "        i = j\n",
    "\n",
    "    # merge tiny segments forward if requested\n",
    "    if min_seg_lines > 1 and segs:\n",
    "        merged: List[Tuple[int, int]] = []\n",
    "        buf_start, buf_end = segs[0]\n",
    "        for (s, e) in segs[1:]:\n",
    "            if (buf_end - buf_start + 1) < min_seg_lines:\n",
    "                # merge with next\n",
    "                buf_end = e\n",
    "            else:\n",
    "                merged.append((buf_start, buf_end))\n",
    "                buf_start, buf_end = s, e\n",
    "        merged.append((buf_start, buf_end))\n",
    "        segs = merged\n",
    "\n",
    "    return segs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = list(iter_jsonl(BASE_JSONL_PATH))\n",
    "print(f\"Loaded {len(records)} records from {BASE_JSONL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b5e19",
   "metadata": {},
   "source": [
    "# ***Main Logic***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2b3a6",
   "metadata": {},
   "source": [
    "### ***Extracting Segment-Level (Code block) Approximated Task Features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskgen = TaskGenerator()\n",
    "embedder = TaskEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7407c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments_meta: List[Dict[str, Any]] = [] # per segment\n",
    "all_segment_vecs: List[np.ndarray] = [] # raw vectors (pre-PCA), one per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA check\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"LLM devices:\", {p.device.type for p in taskgen.model.parameters()})\n",
    "print(\"Embedder devices:\", {p.device.type for p in embedder.model.parameters()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar_docs = tqdm(\n",
    "    records, \n",
    "    desc=\"Docs\", \n",
    "    dynamic_ncols=True, \n",
    "    position=0\n",
    ")\n",
    "\n",
    "open(OUTPUT_RAW_EMBED_PATH, \"w\", encoding=\"utf-8\").close()  # clear file\n",
    "\n",
    "for doc_idx, rec in enumerate(pbar_docs):\n",
    "    doc_key = smart_doc_key(rec)\n",
    "    pid = str(rec.get(\"problem_id\", \"\"))\n",
    "    lines = rec.get(\"lines\") or (rec.get(\"text\") or \"\").splitlines()\n",
    "    lang_hint = LANGUAGE_HINT\n",
    "\n",
    "    segs = segment_code(lines, lang_hint, min_seg_lines=MIN_SEG_LINES)\n",
    "\n",
    "    full_program_text = rec.get(\"text\") or \"\\n\".join(lines)\n",
    "\n",
    "    pbar_segs = tqdm(\n",
    "        total=len(segs),\n",
    "        desc=f\"Doc {doc_idx+1}/{len(records)} segments\",\n",
    "        dynamic_ncols=True,\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        mininterval=0.1,\n",
    "    )\n",
    "\n",
    "    # Build simple neighbor context (optional)\n",
    "    seg_count = 0\n",
    "\n",
    "    # per-document debug cap\n",
    "    debug_prompts_printed = 0\n",
    "\n",
    "    for seg_id, (s, e) in enumerate(segs):\n",
    "        # Assemble prompt inputs\n",
    "        seg_text = \"\\n\".join(lines[s : e + 1])\n",
    "        '''\n",
    "        ctx_parts = []\n",
    "        if SEGMENT_WINDOW_K > 0:\n",
    "            left = max(0, seg_id - SEGMENT_WINDOW_K)\n",
    "            right = min(len(segs) - 1, seg_id + SEGMENT_WINDOW_K)\n",
    "            for k in range(left, right + 1):\n",
    "                if k == seg_id:\n",
    "                    continue\n",
    "                ss, ee = segs[k]\n",
    "                ctx_parts.append(\"\\n\".join(lines[ss : ee + 1]))\n",
    "        ctx_text = \"\\n\\n\".join(ctx_parts)\n",
    "        '''\n",
    "        ctx_text = full_program_text\n",
    "\n",
    "        # Prepare a debug copy of the exact prompt (without chat wrap)\n",
    "        debug_prompt = taskgen.prompt_segment(seg_text, lang_hint, ctx_text)\n",
    "\n",
    "        #print(taskgen.tokenizer.chat_template)\n",
    "        tasks = taskgen.gen_tasks(seg_text, lang_hint, ctx_text)\n",
    "        if not tasks:\n",
    "            tasks = [\"Perform the described code segment task.\"]\n",
    "        vecs = embedder.encode(tasks)\n",
    "        vec = vecs.mean(axis=0)\n",
    "\n",
    "        '''\n",
    "        # debugging statements\n",
    "        if debug_prompts_printed < 5:\n",
    "            print(f\"\\n[DEBUG] doc_key={doc_key} seg_id={seg_id} lines={s}-{e}\")\n",
    "            print(\"----- PROMPT BEGIN -----\")\n",
    "            print(debug_prompt)\n",
    "            print(\"----- PROMPT END -------\")\n",
    "            print(\"----- APPROX TASK(S) -----\")\n",
    "            for t_i, t in enumerate(tasks, 1):\n",
    "                print(f\"{t_i}. {t}\")\n",
    "            print(\"----- END --------------\\n\")\n",
    "            debug_prompts_printed += 1\n",
    "\n",
    "        if seg_count < 20:\n",
    "            print(f\"seg_text ({s}-{e}):============\\n{seg_text}\\n===end of segment===\\n\")\n",
    "            seg_count += 1\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        all_segments_meta.append({\n",
    "            \"doc_key\": doc_key,\n",
    "            \"problem_id\": pid,\n",
    "            \"segment_id\": seg_id,\n",
    "            \"seg_start\": s,\n",
    "            \"seg_end\": e,\n",
    "            \"n_lines\": (e - s + 1),\n",
    "            \"n_tasks\": len(tasks),\n",
    "        })\n",
    "        all_segment_vecs.append(vec)\n",
    "        '''\n",
    "\n",
    "        record = {\n",
    "            \"doc_key\": doc_key,\n",
    "            \"problem_id\": pid,\n",
    "            \"segment_id\": seg_id,\n",
    "            \"seg_start\": s,\n",
    "            \"seg_end\": e,\n",
    "            \"n_lines\": (e - s + 1),\n",
    "            \"n_tasks\": len(tasks),\n",
    "            \"vec_raw\": [float(x) for x in vec.tolist()],\n",
    "        }\n",
    "        append_jsonl(OUTPUT_RAW_EMBED_PATH, [record])\n",
    "\n",
    "        pbar_segs.update(1)\n",
    "\n",
    "    pbar_segs.close()\n",
    "\n",
    "print(f\"Collected {len(all_segment_vecs)} segment vectors across {len(records)} docs.\")\n",
    "print(f\"Saved raw embeddings to {OUTPUT_RAW_EMBED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678007a",
   "metadata": {},
   "source": [
    "## ***!!! Things below are not done yet !!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371dd12",
   "metadata": {},
   "source": [
    "### ***Transferring BERT's RAW 768-dims Embeddings into 128-dims for Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, gc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "# ====== 路径参数（按需改）======\n",
    "RAW_EMB_PATH = os.environ.get(\"OUTPUT_RAW_EMBED_PATH\", \"./limo_atf/segment_python_atfeature_raw_embeddings.jsonl\") # 你给的示例文件，实际跑全量时改回你的 raw embeddings 路径\n",
    "OUT_SEG_PCA_JSONL = os.environ.get(\"OUT_SEG_PCA_JSONL\", \"./limo_atf/segment_python_atfeature_pca128.jsonl\")\n",
    "PCA_PICKLE_PATH = os.environ.get(\"PCA_PICKLE_PATH\", \"./pca/segment_python_atfeature_pca128.pkl\")\n",
    "\n",
    "# ====== PCA 参数（可改）======\n",
    "N_COMPONENTS = 128\n",
    "BATCH_SIZE   = 4096   # 按显存/内存酌情调大/调小\n",
    "DTYPE        = np.float32\n",
    "ROUND_DEC    = 12      # 写 JSON 时保留的小数位，空间 vs 精度自己权衡\n",
    "\n",
    "# 如果只想用“训练集文档”去拟合 PCA（更科学），可以把 doc_key 放在一个 txt 里\n",
    "TRAIN_DOC_KEYS_FILE = os.environ.get(\"TRAIN_DOC_KEYS_FILE\", \"\")  # 可留空\n",
    "if TRAIN_DOC_KEYS_FILE and os.path.exists(TRAIN_DOC_KEYS_FILE):\n",
    "    with open(TRAIN_DOC_KEYS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        TRAIN_DOC_KEYS = set(x.strip() for x in f if x.strip())\n",
    "else:\n",
    "    TRAIN_DOC_KEYS = None\n",
    "\n",
    "def iter_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "def record_to_vec(rec):\n",
    "    # Raw 里是 rec[\"vec_raw\"]，长度通常 768（CodeBERT-base）\n",
    "    return np.asarray(rec[\"vec_raw\"], dtype=DTYPE)\n",
    "\n",
    "def want_for_fit(rec):\n",
    "    # 若提供了训练集 doc_key 白名单，则只用这些样本进行 partial_fit\n",
    "    return (TRAIN_DOC_KEYS is None) or (rec.get(\"doc_key\") in TRAIN_DOC_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca = IncrementalPCA(n_components=N_COMPONENTS)\n",
    "\n",
    "buf = []\n",
    "cnt_fit = 0\n",
    "\n",
    "for rec in tqdm(iter_jsonl(RAW_EMB_PATH), desc=\"Fitting PCA (pass-1)\"):\n",
    "    if not want_for_fit(rec):\n",
    "        continue\n",
    "    v = record_to_vec(rec)\n",
    "    buf.append(v)\n",
    "    if len(buf) == BATCH_SIZE:\n",
    "        X = np.vstack(buf)\n",
    "        ipca.partial_fit(X)\n",
    "        cnt_fit += len(buf)\n",
    "        buf.clear()\n",
    "\n",
    "if buf:\n",
    "    X = np.vstack(buf)\n",
    "    ipca.partial_fit(X)\n",
    "    cnt_fit += len(buf)\n",
    "    buf.clear()\n",
    "\n",
    "print(f\"[PCA] partial_fit samples: {cnt_fit}\")\n",
    "joblib.dump(ipca, PCA_PICKLE_PATH)\n",
    "print(f\"[PCA] saved to: {PCA_PICKLE_PATH}\")\n",
    "\n",
    "# 解释方差占比（大概看下 128 维保留了多少信息）\n",
    "explained = getattr(ipca, \"explained_variance_ratio_\", None)\n",
    "if explained is not None:\n",
    "    print(f\"[PCA] explained variance (sum of 128): {explained.sum():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先清空输出文件\n",
    "os.makedirs(os.path.dirname(OUT_SEG_PCA_JSONL), exist_ok=True)\n",
    "open(OUT_SEG_PCA_JSONL, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "buf = []\n",
    "meta_buf = []\n",
    "written = 0\n",
    "\n",
    "def flush_transform_write():\n",
    "    global written\n",
    "    if not buf: \n",
    "        return\n",
    "    X = np.vstack(buf)\n",
    "    Z = ipca.transform(X).astype(DTYPE)       # [batch, 128]\n",
    "    with open(OUT_SEG_PCA_JSONL, \"a\", encoding=\"utf-8\") as wf:\n",
    "        for meta, z in zip(meta_buf, Z):\n",
    "            out = {\n",
    "                # —— 保留你后面可能会用到的定位信息 —— \n",
    "                \"doc_key\":   meta[\"doc_key\"],\n",
    "                \"problem_id\":meta[\"problem_id\"],\n",
    "                \"segment_id\":meta[\"segment_id\"],\n",
    "                \"seg_start\": meta[\"seg_start\"],\n",
    "                \"seg_end\":   meta[\"seg_end\"],\n",
    "                \"n_lines\":   meta[\"n_lines\"],\n",
    "                # —— 新的 128D 向量 —— \n",
    "                \"vec_atc128\":[round(float(x), ROUND_DEC) for x in z.tolist()],\n",
    "            }\n",
    "            wf.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    buf.clear()\n",
    "    meta_buf.clear()\n",
    "    gc.collect()\n",
    "\n",
    "for rec in tqdm(iter_jsonl(RAW_EMB_PATH), desc=\"Transform & Write (pass-2)\"):\n",
    "    v = record_to_vec(rec)\n",
    "    buf.append(v)\n",
    "    # 只保留必要的元信息，原始 vec_raw 不再写回\n",
    "    meta_buf.append({\n",
    "        \"doc_key\":   rec.get(\"doc_key\"),\n",
    "        \"problem_id\":rec.get(\"problem_id\"),\n",
    "        \"segment_id\":rec.get(\"segment_id\"),\n",
    "        \"seg_start\": rec.get(\"seg_start\"),\n",
    "        \"seg_end\":   rec.get(\"seg_end\"),\n",
    "        \"n_lines\":   rec.get(\"n_lines\"),\n",
    "    })\n",
    "    if len(buf) == BATCH_SIZE:\n",
    "        flush_transform_write()\n",
    "\n",
    "flush_transform_write()\n",
    "print(f\"[WRITE] segments written: {written}\")\n",
    "print(f\"[OUT] {OUT_SEG_PCA_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "IN_PCA_JSONL = \"./limo_atf/segment_python_atfeature_pca128.jsonl\"\n",
    "\n",
    "n, bad = 0, 0\n",
    "for line in tqdm(open(IN_PCA_JSONL, \"r\", encoding=\"utf-8\"), desc=\"Check 128-d\"):\n",
    "    rec = json.loads(line)\n",
    "    v = rec.get(\"vec_atc128\", [])\n",
    "    if not isinstance(v, list) or len(v) != 128:\n",
    "        bad += 1\n",
    "    n += 1\n",
    "\n",
    "print(f\"total records: {n}, bad: {bad}\")\n",
    "assert bad == 0, \"发现不是128维的记录，请先清洗掉这些行\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment→line（sidecar）生成器：按照 ccfeatures 策略\n",
    "import os, json, hashlib\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== 输入：你已生成的 128 维 segment 级 JSONL =====\n",
    "IN_PCA_JSONL = \"./limo_atf/segment_python_atfeature_pca128.jsonl\"  # 改成你的路径也行\n",
    "\n",
    "# ===== 输出：与 ccfeatures 风格一致的 sidecar 目录结构 =====\n",
    "OUT_DIR = \"./limo_atf/great_data\"         # 每个文档会有一个 .npy\n",
    "INDEX_JSON = os.path.join(OUT_DIR, \"index.json\")\n",
    "\n",
    "# ===== 行为参数 =====\n",
    "EMB_DIM = 128                 # 你已确定 128\n",
    "DTYPE_SAVE = \"float32\"        # 可选 \"float32\" / \"float16\"\n",
    "OVERLAP_MODE = \"average\"      # \"average\"（推荐，与 ccfeatures 一致无歧义）或 \"last\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def _safe_key(doc_key: str) -> str:\n",
    "    \"\"\"把 doc_key 转成稳定可用的文件名（避免奇怪字符/过长）。\"\"\"\n",
    "    return hashlib.sha1(doc_key.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def _as_dtype(arr: np.ndarray, dtype_name: str) -> np.ndarray:\n",
    "    return arr.astype(np.float16 if dtype_name == \"float16\" else np.float32, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第 1 遍：把同一 doc_key 的 segment 收集起来，并统计每个 doc 的最大行号（决定矩阵高度）\n",
    "groups = defaultdict(list)   # doc_key -> [(s, e, vec128), ...]\n",
    "meta   = {}                  # doc_key -> {\"problem_id\": ..., \"max_end\": ...}\n",
    "\n",
    "with open(IN_PCA_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Group segments by doc_key\"):\n",
    "        rec = json.loads(line)\n",
    "        dk  = rec[\"doc_key\"]\n",
    "        s, e = int(rec[\"seg_start\"]), int(rec[\"seg_end\"])\n",
    "        vec = np.asarray(rec[\"vec_atc128\"], dtype=np.float32)  # 先用 fp32 聚合更稳\n",
    "        if vec.shape[0] != EMB_DIM:\n",
    "            raise ValueError(f\"Expect {EMB_DIM}-D, got {vec.shape} for doc_key={dk}\")\n",
    "        groups[dk].append((s, e, vec))\n",
    "        info = meta.setdefault(dk, {\n",
    "            \"problem_id\": rec.get(\"problem_id\", \"\"),\n",
    "            \"max_end\": -1\n",
    "        })\n",
    "        if e > info[\"max_end\"]:\n",
    "            info[\"max_end\"] = e\n",
    "\n",
    "print(f\"docs collected: {len(groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第 2 遍：对每个 doc 做广播\n",
    "index = {}\n",
    "for dk, segs in tqdm(groups.items(), desc=\"Write per-doc line npy\"):\n",
    "    L = int(meta[dk][\"max_end\"]) + 1\n",
    "    if L <= 0:\n",
    "        # 空文档，落一个空矩阵也行；这里直接跳过（看你数据是否存在这种情况）\n",
    "        continue\n",
    "\n",
    "    # A: 行级累加矩阵；C: 计数矩阵（用于平均）\n",
    "    A = np.zeros((L, EMB_DIM), dtype=np.float32)\n",
    "    C = np.zeros((L, 1),       dtype=np.int32)\n",
    "\n",
    "    if OVERLAP_MODE == \"average\":\n",
    "        # 对每个 segment 的范围，做向量累加 & 计数 +1\n",
    "        for s, e, vec in segs:\n",
    "            s = max(0, s)\n",
    "            e = min(L - 1, e)\n",
    "            if e < s: \n",
    "                continue\n",
    "            A[s:e+1, :] += vec\n",
    "            C[s:e+1, :] += 1\n",
    "\n",
    "        # 把累加结果除以计数，得到平均；无覆盖的行为 0（C=0）\n",
    "        mask = (C > 0)\n",
    "        A[mask[:, 0]] /= C[mask][:, None]\n",
    "\n",
    "    elif OVERLAP_MODE == \"last\":\n",
    "        # 直接覆盖：后面的 segment 覆盖前面的（不建议，除非你确认没有重叠）\n",
    "        for s, e, vec in segs:\n",
    "            s = max(0, s)\n",
    "            e = min(L - 1, e)\n",
    "            if e < s:\n",
    "                continue\n",
    "            A[s:e+1, :] = vec\n",
    "    else:\n",
    "        raise ValueError(\"OVERLAP_MODE must be 'average' or 'last'\")\n",
    "\n",
    "    # 保存为 .npy（体积可选 float16/float32）\n",
    "    key = _safe_key(dk)\n",
    "    npy_path = os.path.join(OUT_DIR, f\"{key}.npy\")\n",
    "    np.save(npy_path, _as_dtype(A, DTYPE_SAVE))\n",
    "\n",
    "    # 写入索引\n",
    "    index[dk] = {\n",
    "        \"problem_id\": meta[dk][\"problem_id\"],\n",
    "        \"n_lines\": int(L),\n",
    "        \"dim\": EMB_DIM,\n",
    "        \"dtype\": DTYPE_SAVE,\n",
    "        \"path\": npy_path\n",
    "    }\n",
    "\n",
    "# 总索引\n",
    "with open(INDEX_JSON, \"w\", encoding=\"utf-8\") as wf:\n",
    "    json.dump({\n",
    "        \"created_at\": datetime.utcnow().isoformat()+\"Z\",\n",
    "        \"overlap_mode\": OVERLAP_MODE,\n",
    "        \"docs\": index\n",
    "    }, wf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[DONE] docs: {len(index)}\")\n",
    "print(f\"[INDEX] {INDEX_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 sidecar 中某个 doc_key 的行级 AT 矩阵\n",
    "def load_atc_sidecar(index_json: str, doc_key: str):\n",
    "    with open(index_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        idx = json.load(f)\n",
    "    info = idx[\"docs\"][doc_key]\n",
    "    A = np.load(info[\"path\"])   # shape: [n_lines, 128]，dtype 可能是 fp16/fp32\n",
    "    return A, info\n",
    "\n",
    "# 和 ccfeature 在最后一维拼接（按最小长度对齐）\n",
    "def concat_cc_and_atc(cc_mat: np.ndarray, at_mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    cc_mat: [L1, CcDim]\n",
    "    at_mat: [L2, 128]\n",
    "    返回: [min(L1,L2), CcDim+128]\n",
    "    \"\"\"\n",
    "    L = min(cc_mat.shape[0], at_mat.shape[0])\n",
    "    if at_mat.dtype != np.float32:\n",
    "        at_mat = at_mat.astype(np.float32, copy=False)\n",
    "    return np.concatenate([cc_mat[:L], at_mat[:L]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a27cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机抽一个 doc 做检查\n",
    "any_doc = next(iter(index))\n",
    "A = np.load(index[any_doc][\"path\"])\n",
    "print(any_doc, A.shape, A.dtype, A.min(), A.max())\n",
    "assert A.shape[1] == 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a76ed",
   "metadata": {},
   "source": [
    "### ***Below is for generating 256dim `at_features`, not 128***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890ea7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, gc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import joblib\n",
    "\n",
    "# ====== 路径参数（按需改）======\n",
    "RAW_EMB_PATH = os.environ.get(\"OUTPUT_RAW_EMBED_PATH\", \"./limo_atf/segment_python_atfeature_raw_embeddings.jsonl\") # raw embeddings 路径\n",
    "OUT_SEG_PCA_JSONL = os.environ.get(\"OUT_SEG_PCA_JSONL\", \"./limo_atf/segment_python_atfeature_pca256.jsonl\")\n",
    "PCA_PICKLE_PATH = os.environ.get(\"PCA_PICKLE_PATH\", \"./pca/segment_python_atfeature_pca256.pkl\")\n",
    "\n",
    "# ====== PCA 参数（可改）======\n",
    "N_COMPONENTS = 256\n",
    "BATCH_SIZE   = 4096   # 按显存/内存酌情调大/调小\n",
    "DTYPE        = np.float32\n",
    "ROUND_DEC    = 12      # 写 JSON 时保留的小数位，空间 vs 精度自己权衡\n",
    "\n",
    "# 如果只想用“训练集文档”去拟合 PCA（更科学），可以把 doc_key 放在一个 txt 里\n",
    "TRAIN_DOC_KEYS_FILE = os.environ.get(\"TRAIN_DOC_KEYS_FILE\", \"\")  # 可留空\n",
    "if TRAIN_DOC_KEYS_FILE and os.path.exists(TRAIN_DOC_KEYS_FILE):\n",
    "    with open(TRAIN_DOC_KEYS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        TRAIN_DOC_KEYS = set(x.strip() for x in f if x.strip())\n",
    "else:\n",
    "    TRAIN_DOC_KEYS = None\n",
    "\n",
    "def iter_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "def record_to_vec(rec):\n",
    "    # Raw 里是 rec[\"vec_raw\"]，长度通常 768（CodeBERT-base）\n",
    "    return np.asarray(rec[\"vec_raw\"], dtype=DTYPE)\n",
    "\n",
    "def want_for_fit(rec):\n",
    "    # 若提供了训练集 doc_key 白名单，则只用这些样本进行 partial_fit\n",
    "    return (TRAIN_DOC_KEYS is None) or (rec.get(\"doc_key\") in TRAIN_DOC_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5843adb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65006492c64492db0f9829d098756ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting PCA (pass-1): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] partial_fit samples: 13794\n",
      "[PCA] saved to: ./pca/segment_python_atfeature_pca256.pkl\n",
      "[PCA] explained variance (sum of 256): 0.9713\n"
     ]
    }
   ],
   "source": [
    "ipca = IncrementalPCA(n_components=N_COMPONENTS)\n",
    "\n",
    "buf = []\n",
    "cnt_fit = 0\n",
    "\n",
    "for rec in tqdm(iter_jsonl(RAW_EMB_PATH), desc=\"Fitting PCA (pass-1)\"):\n",
    "    if not want_for_fit(rec):\n",
    "        continue\n",
    "    v = record_to_vec(rec)\n",
    "    buf.append(v)\n",
    "    if len(buf) == BATCH_SIZE:\n",
    "        X = np.vstack(buf)\n",
    "        ipca.partial_fit(X)\n",
    "        cnt_fit += len(buf)\n",
    "        buf.clear()\n",
    "\n",
    "if buf:\n",
    "    X = np.vstack(buf)\n",
    "    ipca.partial_fit(X)\n",
    "    cnt_fit += len(buf)\n",
    "    buf.clear()\n",
    "\n",
    "print(f\"[PCA] partial_fit samples: {cnt_fit}\")\n",
    "joblib.dump(ipca, PCA_PICKLE_PATH)\n",
    "print(f\"[PCA] saved to: {PCA_PICKLE_PATH}\")\n",
    "\n",
    "# 解释方差占比（大概看下 256 维保留了多少信息）\n",
    "explained = getattr(ipca, \"explained_variance_ratio_\", None)\n",
    "if explained is not None:\n",
    "    print(f\"[PCA] explained variance (sum of 256): {explained.sum():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef6e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11ad55437f14fe3bbd54c0d905d7a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transform & Write (pass-2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WRITE] segments written: 13794\n",
      "[OUT] ./limo_atf/segment_python_atfeature_pca256.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 先清空输出文件\n",
    "os.makedirs(os.path.dirname(OUT_SEG_PCA_JSONL), exist_ok=True)\n",
    "open(OUT_SEG_PCA_JSONL, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "buf = []\n",
    "meta_buf = []\n",
    "written = 0\n",
    "\n",
    "def flush_transform_write():\n",
    "    global written\n",
    "    if not buf: \n",
    "        return\n",
    "    X = np.vstack(buf)\n",
    "    Z = ipca.transform(X).astype(DTYPE)       # [batch, 256]\n",
    "    with open(OUT_SEG_PCA_JSONL, \"a\", encoding=\"utf-8\") as wf:\n",
    "        for meta, z in zip(meta_buf, Z):\n",
    "            out = {\n",
    "                # —— 保留你后面可能会用到的定位信息 —— \n",
    "                \"doc_key\":   meta[\"doc_key\"],\n",
    "                \"problem_id\":meta[\"problem_id\"],\n",
    "                \"segment_id\":meta[\"segment_id\"],\n",
    "                \"seg_start\": meta[\"seg_start\"],\n",
    "                \"seg_end\":   meta[\"seg_end\"],\n",
    "                \"n_lines\":   meta[\"n_lines\"],\n",
    "                # —— 新的 256D 向量 —— \n",
    "                \"vec_atc256\":[round(float(x), ROUND_DEC) for x in z.tolist()],\n",
    "            }\n",
    "            wf.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "            written += 1\n",
    "    buf.clear()\n",
    "    meta_buf.clear()\n",
    "    gc.collect()\n",
    "\n",
    "for rec in tqdm(iter_jsonl(RAW_EMB_PATH), desc=\"Transform & Write (pass-2)\"):\n",
    "    v = record_to_vec(rec)\n",
    "    buf.append(v)\n",
    "    # 只保留必要的元信息，原始 vec_raw 不再写回\n",
    "    meta_buf.append({\n",
    "        \"doc_key\":   rec.get(\"doc_key\"),\n",
    "        \"problem_id\":rec.get(\"problem_id\"),\n",
    "        \"segment_id\":rec.get(\"segment_id\"),\n",
    "        \"seg_start\": rec.get(\"seg_start\"),\n",
    "        \"seg_end\":   rec.get(\"seg_end\"),\n",
    "        \"n_lines\":   rec.get(\"n_lines\"),\n",
    "    })\n",
    "    if len(buf) == BATCH_SIZE:\n",
    "        flush_transform_write()\n",
    "\n",
    "flush_transform_write()\n",
    "print(f\"[WRITE] segments written: {written}\")\n",
    "print(f\"[OUT] {OUT_SEG_PCA_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39225c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63d6203437c4c6f8dce12768f3d2c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Check 256-d: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records: 13794, bad: 0\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "IN_PCA_JSONL = \"./limo_atf/segment_python_atfeature_pca256.jsonl\"\n",
    "\n",
    "n, bad = 0, 0\n",
    "for line in tqdm(open(IN_PCA_JSONL, \"r\", encoding=\"utf-8\"), desc=\"Check 256-d\"):\n",
    "    rec = json.loads(line)\n",
    "    v = rec.get(\"vec_atc256\", [])\n",
    "    if not isinstance(v, list) or len(v) != 256:\n",
    "        bad += 1\n",
    "    n += 1\n",
    "\n",
    "print(f\"total records: {n}, bad: {bad}\")\n",
    "assert bad == 0, \"发现不是256维的记录, 先清洗掉这些行\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03522fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment→line（sidecar）生成器：按照 ccfeatures 策略\n",
    "import os, json, hashlib\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== 输入：已生成的 256 维 segment 级 JSONL =====\n",
    "IN_PCA_JSONL = \"./limo_atf/segment_python_atfeature_pca256.jsonl\"  # 改成你的路径也行\n",
    "\n",
    "# ===== 输出：与 ccfeatures 风格一致的 sidecar 目录结构 =====\n",
    "OUT_DIR = \"./limo_atf/great_data_256\"         # 每个文档会有一个 .npy\n",
    "INDEX_JSON = os.path.join(OUT_DIR, \"index.json\")\n",
    "\n",
    "# ===== 行为参数 =====\n",
    "EMB_DIM = 256                 # 你已确定 256\n",
    "DTYPE_SAVE = \"float32\"        # 可选 \"float32\" / \"float16\"\n",
    "OVERLAP_MODE = \"average\"      # \"average\"（推荐，与 ccfeatures 一致无歧义）或 \"last\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def _safe_key(doc_key: str) -> str:\n",
    "    \"\"\"把 doc_key 转成稳定可用的文件名（避免奇怪字符/过长）。\"\"\"\n",
    "    return hashlib.sha1(doc_key.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def _as_dtype(arr: np.ndarray, dtype_name: str) -> np.ndarray:\n",
    "    return arr.astype(np.float16 if dtype_name == \"float16\" else np.float32, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804ccc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9b0cad132243c68f91e1e5bef60ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Group segments by doc_key: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs collected: 2752\n"
     ]
    }
   ],
   "source": [
    "# 第 1 遍：把同一 doc_key 的 segment 收集起来，并统计每个 doc 的最大行号（决定矩阵高度）\n",
    "groups = defaultdict(list)   # doc_key -> [(s, e, vec256), ...]\n",
    "meta   = {}                  # doc_key -> {\"problem_id\": ..., \"max_end\": ...}\n",
    "\n",
    "with open(IN_PCA_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Group segments by doc_key\"):\n",
    "        rec = json.loads(line)\n",
    "        dk  = rec[\"doc_key\"]\n",
    "        s, e = int(rec[\"seg_start\"]), int(rec[\"seg_end\"])\n",
    "        vec = np.asarray(rec[\"vec_atc256\"], dtype=np.float32)  # 先用 fp32 聚合更稳\n",
    "        if vec.shape[0] != EMB_DIM:\n",
    "            raise ValueError(f\"Expect {EMB_DIM}-D, got {vec.shape} for doc_key={dk}\")\n",
    "        groups[dk].append((s, e, vec))\n",
    "        info = meta.setdefault(dk, {\n",
    "            \"problem_id\": rec.get(\"problem_id\", \"\"),\n",
    "            \"max_end\": -1\n",
    "        })\n",
    "        if e > info[\"max_end\"]:\n",
    "            info[\"max_end\"] = e\n",
    "\n",
    "print(f\"docs collected: {len(groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867cd4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8673bd14d57496dac1b969ab098d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Write per-doc line npy:   0%|          | 0/2752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] docs: 2752\n",
      "[INDEX] ./limo_atf/great_data_256/index.json\n"
     ]
    }
   ],
   "source": [
    "# 第 2 遍：对每个 doc 做广播\n",
    "index = {}\n",
    "for dk, segs in tqdm(groups.items(), desc=\"Write per-doc line npy\"):\n",
    "    L = int(meta[dk][\"max_end\"]) + 1\n",
    "    if L <= 0:\n",
    "        # 空文档，落一个空矩阵也行；这里直接跳过（看你数据是否存在这种情况）\n",
    "        continue\n",
    "\n",
    "    # A: 行级累加矩阵；C: 计数矩阵（用于平均）\n",
    "    A = np.zeros((L, EMB_DIM), dtype=np.float32)\n",
    "    C = np.zeros((L, 1),       dtype=np.int32)\n",
    "\n",
    "    if OVERLAP_MODE == \"average\":\n",
    "        # 对每个 segment 的范围，做向量累加 & 计数 +1\n",
    "        for s, e, vec in segs:\n",
    "            s = max(0, s)\n",
    "            e = min(L - 1, e)\n",
    "            if e < s: \n",
    "                continue\n",
    "            A[s:e+1, :] += vec\n",
    "            C[s:e+1, :] += 1\n",
    "\n",
    "        # 把累加结果除以计数，得到平均；无覆盖的行为 0（C=0）\n",
    "        mask = (C > 0)\n",
    "        A[mask[:, 0]] /= C[mask][:, None]\n",
    "\n",
    "    elif OVERLAP_MODE == \"last\":\n",
    "        # 直接覆盖：后面的 segment 覆盖前面的（不建议，除非你确认没有重叠）\n",
    "        for s, e, vec in segs:\n",
    "            s = max(0, s)\n",
    "            e = min(L - 1, e)\n",
    "            if e < s:\n",
    "                continue\n",
    "            A[s:e+1, :] = vec\n",
    "    else:\n",
    "        raise ValueError(\"OVERLAP_MODE must be 'average' or 'last'\")\n",
    "\n",
    "    # 保存为 .npy（体积可选 float16/float32）\n",
    "    key = _safe_key(dk)\n",
    "    npy_path = os.path.join(OUT_DIR, f\"{key}.npy\")\n",
    "    np.save(npy_path, _as_dtype(A, DTYPE_SAVE))\n",
    "\n",
    "    # 写入索引\n",
    "    index[dk] = {\n",
    "        \"problem_id\": meta[dk][\"problem_id\"],\n",
    "        \"n_lines\": int(L),\n",
    "        \"dim\": EMB_DIM,\n",
    "        \"dtype\": DTYPE_SAVE,\n",
    "        \"path\": npy_path\n",
    "    }\n",
    "\n",
    "# 总索引\n",
    "with open(INDEX_JSON, \"w\", encoding=\"utf-8\") as wf:\n",
    "    json.dump({\n",
    "        \"created_at\": datetime.utcnow().isoformat()+\"Z\",\n",
    "        \"overlap_mode\": OVERLAP_MODE,\n",
    "        \"docs\": index\n",
    "    }, wf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[DONE] docs: {len(index)}\")\n",
    "print(f\"[INDEX] {INDEX_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5965ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 sidecar 中某个 doc_key 的行级 AT 矩阵\n",
    "def load_atc_sidecar(index_json: str, doc_key: str):\n",
    "    with open(index_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        idx = json.load(f)\n",
    "    info = idx[\"docs\"][doc_key]\n",
    "    A = np.load(info[\"path\"])   # shape: [n_lines, 256]，dtype 可能是 fp16/fp32\n",
    "    return A, info\n",
    "\n",
    "# 和 ccfeature 在最后一维拼接（按最小长度对齐）\n",
    "def concat_cc_and_atc(cc_mat: np.ndarray, at_mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    cc_mat: [L1, CcDim]\n",
    "    at_mat: [L2, 256]\n",
    "    返回: [min(L1,L2), CcDim+256]\n",
    "    \"\"\"\n",
    "    L = min(cc_mat.shape[0], at_mat.shape[0])\n",
    "    if at_mat.dtype != np.float32:\n",
    "        at_mat = at_mat.astype(np.float32, copy=False)\n",
    "    return np.concatenate([cc_mat[:L], at_mat[:L]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b860cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p03166::d5d6e3d0311f85dd9eeb15a32b9f1c8a7f90b330 (30, 256) float32 -2.2408905 2.780463\n"
     ]
    }
   ],
   "source": [
    "# 随机抽一个 doc 做检查\n",
    "any_doc = next(iter(index))\n",
    "A = np.load(index[any_doc][\"path\"])\n",
    "print(any_doc, A.shape, A.dtype, A.min(), A.max())\n",
    "assert A.shape[1] == 256\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
